{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course Summary CSCI191t\n",
    "\n",
    "\n",
    "## Introduction: \n",
    "This course has covered, many interesting topics and opened my eyes to new avenues of coding \n",
    "langauges and techniques I can use to program with. This course has introduced us to Jupyter Notebook integrated \n",
    "with python and github (which i never knew about), showed us how to use simple libraries such as pandas,numpy, and \n",
    "scikit learn. Not only are these libraries used for extensive computing and powerful neural network libraries, but \n",
    "libraries such as matplotlib - matplotlib.style.use('ggplot'), graphviz, and sklearn.tree.graph, provides the user \n",
    "with clean, organized, and well layed out data. Making the code easy to follow, and providing visualization to \n",
    "readers that have a hard time maybe understanding one's code / concept. These visualizations really puts an aspect \n",
    "to programming, that allows people to easily extract and work with your existing code / data. Overall this course \n",
    "has showed and given me tools that I am sure I can put to use in the future, and even if I am unable to, is given \n",
    "me new way of thinking, with considering how manipulate data.\n",
    "\n",
    "Following are fragments of code that showed what we learn in each Project. Projects worked on and submitted were:\n",
    "\n",
    "1.Titanic Competition\n",
    "\n",
    "2.Digit Recognizer / MNIST\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Titanic Competiton and what we learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#I have included these first lines of code because this is the foundation of which we have built this project on.\n",
    "#Constantly learning what these libraries can do and how to use them effeciently was extensive and quite fun.\n",
    "#Also implementing full fleshed data representation was great, and gave a visual of a datamining set and what to\n",
    "#for in patterns.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "#Below here we traverse and explore data to see what we really need, and by doing this we are able to exploit\n",
    "#and manipulate data to give us the best results!!!\n",
    "survived_class = dataset[dataset['Survived']==1]['Pclass'].value_counts()\n",
    "dead_class = dataset[dataset['Survived']==0]['Pclass'].value_counts()\n",
    "df_class = pd.DataFrame([survived_class,dead_class])\n",
    "df_class.index = ['Survived','Died']\n",
    "df_class.plot(kind='bar',stacked=True, figsize=(5,3), title=\"Survived/Died by Class\")\n",
    "\n",
    "Class1_survived= df_class.iloc[0,0]/df_class.iloc[:,0].sum()*100\n",
    "Class2_survived = df_class.iloc[0,1]/df_class.iloc[:,1].sum()*100\n",
    "Class3_survived = df_class.iloc[0,2]/df_class.iloc[:,2].sum()*100\n",
    "print(\"Percentage of Class 1 that survived:\" ,round(Class1_survived),\"%\")\n",
    "print(\"Percentage of Class 2 that survived:\" ,round(Class2_survived), \"%\")\n",
    "print(\"Percentage of Class 3 that survived:\" ,round(Class3_survived), \"%\")\n",
    "\n",
    "\n",
    "\n",
    "#Here we conduct Classifer Logistic Regerssion, HOWEVER THERE ARE MANY DIFFERANT CLASSIFIER TECHNIQUES WE CAN USE\n",
    "# For Example WE CAN USE K-NN,Bayes, and RANDOM FOREST.\n",
    "# We use the K-Fold Cross Validation however to \"tighten\" the data.\n",
    "\n",
    "#-----------------------Logistic Regression---------------------------------------------\n",
    "# Fitting Logistic Regression to the Training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(penalty='l2',random_state = 0)\n",
    "\n",
    "# Applying k-Fold Cross Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = classifier, X=X , y=y , cv = 10)\n",
    "print(\"Logistic Regression:\\n Accuracy:\", accuracies.mean(), \"+/-\", accuracies.std(),\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "#-----------------------------------K-NN --------------------------------------------------\n",
    "\n",
    "# Fitting K-NN to the Training set\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier(n_neighbors = 9, metric = 'minkowski', p = 2)\n",
    "\n",
    "\n",
    "# Applying k-Fold Cross Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = classifier, X=X , y=y , cv = 10)\n",
    "print(\"K-NN:\\n Accuracy:\", accuracies.mean(), \"+/-\", accuracies.std(),\"\\n\")\n",
    "\n",
    "\n",
    "#---------------------------------Naive Bayes-------------------------------------------\n",
    "\n",
    "# Fitting Naive Bayes to the Training set\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "\n",
    "# Applying k-Fold Cross Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = classifier, X=X , y=y , cv = 10)\n",
    "print(\"Naive Bayes:\\n Accuracy:\", accuracies.mean(), \"+/-\", accuracies.std(),\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------Random Forest------------------------------------------\n",
    "\n",
    "# Fitting Random Forest Classification to the Training set\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators = 100, criterion = 'entropy', random_state = 0)\n",
    "\n",
    "# Applying k-Fold Cross Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = classifier, X=X , y=y , cv = 10)\n",
    "print(\"Random Forest:\\n Accuracy:\", accuracies.mean(), \"+/-\", accuracies.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Digit Recognizer/MNIST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We begin by loading in some useful R libraries\n",
    "\n",
    "library(randomForest)\n",
    "\n",
    "#We then read the data and seperate the labels from the pixel data\n",
    "train = read.csv(\"train.csv\")\n",
    "test= read.csv(\"test.csv\")\n",
    "\n",
    "labels= as.factor(train[ ,1])\n",
    "train = train[ ,2:785]\n",
    "\n",
    "\n",
    "#Next We Create some potentially useful meta features for training. \n",
    "#Each training case is a vector of a requested pixel intensity values, that range from 0-256 or another desired\n",
    "#number that should be static and given when doing a kaggle competition.\n",
    "#Total pixel intensity\n",
    "total_inensity = rowSums(train)\n",
    "#Number of pixels with non-zero intensity\n",
    "non_zero = rowSums(train!=0)\n",
    "#Average pixel intensity\n",
    "average_inensity = total_inensity/non_zero\n",
    "#Number of low inensity pixels\n",
    "between_0_100 = rowSums(train>0 & train<100)\n",
    "#Proportion of low inensity pixels\n",
    "prop_between_0_100 = between_0_100/non_zero\n",
    "\n",
    "\n",
    "#We then put attributes / features in the data frame\n",
    "new_train=data.frame(\"total_inensity\"=total_inensity,\"non_zero\"=non_zero,\n",
    "                     \"average_inensity\"=average_inensity,\"between_0_100\"=between_0_100,\n",
    "                     \"prop_between_0_100\"=prop_between_0_100)\n",
    "\n",
    "\n",
    "\n",
    "#We then create new features that attempt to capture some of the structure of the images. \n",
    "#We make 28 features for the proportion of an image's total intensity contained in each 1 pixel row do the same \n",
    "#for each 1 pixel column. We create 16 features based on intensity in each large patch when splitting the\n",
    "#image into a 4x4 grid and 49 features based on intensity of each small section when splitting an image \n",
    "#into a 7x7 grid. This reduces the total number of features from 784 to 126 The Goal of these features is to reduce\n",
    "#feature space with without invalidating too much useful information.\n",
    "vertical_seq = seq(1,784,by=28)\n",
    "\n",
    "\n",
    "for (x in 1:28){ #Features for proportion of total pixel intensity in horizontal strips however these can be rotated\n",
    "  row_intensity = rowSums(train[,(1+(28*(x-1))):(28+(28*(x-1)))])\n",
    "  new_train[paste(\"h\",x,sep=\"\")] = row_intensity/total_inensity\n",
    "}\n",
    "\n",
    "#Features for proportion of total pixel intensity in vertical strips\n",
    "for (x in 1:28){\n",
    "  new_train[paste(\"v\",x,sep=\"\")] = rowSums(train[,vertical_seq+(x-1)])/total_inensity\n",
    "}\n",
    "\n",
    "#Features for proportion of total pixel intensity in large patches (splits the image into a 4x4 grid and finds the proportion of total pixel inensity in each section)\n",
    "for (x in 1:4){\n",
    "  for (y in 1:4){\n",
    "    line = 1:7 + ((x-1)*7) + ((y-1)*196)\n",
    "    for (z in 1:6){\n",
    "      line=c(line,line[1:7]+(28*z))\n",
    "    }\n",
    "    new_train[paste(\"section\",x,y,sep=\"\")] = rowSums(train[line])/total_inensity\n",
    "  }\n",
    "}\n",
    "\n",
    "#Features for proportion of total pixel intensity in small patches (splits the image into a 7x7 grid and finds the proportion of total pixel inensity in each section)\n",
    "for (x in 1:7){\n",
    "  for (y in 1:7){\n",
    "    line = 1:4 + ((x-1)*4) + ((y-1)*112)\n",
    "    for (z in 1:3){\n",
    "      line=c(line,line[1:4]+(28*z))\n",
    "    }\n",
    "    new_train[paste(\"smallsection\",x,y,sep=\"\")] = rowSums(train[line])/total_inensity\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Random forest model, and Here is we being training a random forest model, mostly for the reason that \n",
    "#it also did so well in the titanic kaggle competition... so from now on we will mostly be using\n",
    "#the random forest model\n",
    "set.seed(12)\n",
    "rf_mod1= randomForest(labels[1:30000]~., data=part_train, nodesize=1, ntree=250, mtry=5)\n",
    "\n",
    "rf_pred = predict(rf_mod1,newdata=valid)\n",
    "\n",
    "confusionMatrix(rf_pred,labels[30001:42000])\n",
    "\n",
    "\n",
    "\n",
    "#We can also include a stochastic gradient boosting model using the caret package\n",
    "#However that may be too much.\n",
    "#AGAIN THESE ARE CODE FRAGMENTS OF THE MAIN POINTS OF INTEREST IN THE CODE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
